import pickle
import sys
import numpy as np
from math import sqrt
import scipy
import torch
import torch.optim as optim
from sklearn.metrics import roc_auc_score, precision_score, recall_score,precision_recall_curve, auc
from data_merge import data_load
from network.model_ori import Representation_model
torch.multiprocessing.set_start_method('spawn')
from metric import *
from transformers import T5Tokenizer, T5EncoderModel
from sklearn.metrics import matthews_corrcoef

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
prot_tokenizer = T5Tokenizer.from_pretrained('prot_t5_xl_uniref50', do_lower_case=False)
prot_model = T5EncoderModel.from_pretrained("prot_t5_xl_uniref50").to(device)
prot_model.to(torch.float32)

def sequence_feature(sequences):
    protein_input = prot_tokenizer.batch_encode_plus([" ".join(sequences)], add_special_tokens=True, padding=True)
    p_IDS = torch.tensor(protein_input["input_ids"]).to(device)
    p_a_m = torch.tensor(protein_input["attention_mask"]).to(device)
    with torch.no_grad():
        prot_outputs = prot_model(input_ids=p_IDS, attention_mask=p_a_m)
    prot_feature = prot_outputs.last_hidden_state.squeeze(0).to('cpu').data.numpy()
    return prot_feature

if __name__ == "__main__":
    batchs = 1
    """CPU or GPU."""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print('The code uses GPU...')
    else:
        device = torch.device('cpu')
        print('The code uses CPU!!!')

    model = Representation_model(3, 512, 256, 64, 128, batchs, device).to(device)
    target_model_state_dict = model.state_dict()
    model_state_dict = torch.load("output/model/pretrain_ones_finetune5") # RNA_RNA-545_RNA-161_1 DNA_DNA-573_DNA-129_1
    model.load_state_dict(model_state_dict)
    ache_sequence = "SELLVNTKSGKVMGTRVPVLSSHISAFLGIPFAEPPVGNMRFRRPEPKKPWSGVWNASTYPNNCQQYVDEQFPGFSGSEMWNPNREMSEDCLYLNIWVPSPRPKSTTVMVWIYGGGFYSGSSTLDVYNGKYLAYTEEVVLVSLSYRVGAFGFLALHGSQEAPGNVGLLDQRMALQWVHDNIQFFGGDPKTVTIFGESAGGASVGMHILSPGSRDLFRRAILQSGSPNCPWASVSVAEGRRRAVELGRNLNCNLNSDEELIHCLREKKPQELIDVEWNVLPFDSIFRFSFVPVIDGEFFPTSLESMLNSGNFKKTQILLGVNKDEGSFFLLYGAPGFSKDSESKISREDFMSGVKLSVPHANDLGLDAVTLQYTDWMDDNNGIKNRDGLDDIVGDHNVICPLMHFVNKYTKFGNGTYLYFFNHRASNLVWPEWMGVIHGYEIEFVFGLPLVKELNYTAEEEALSRRIMHYWATFAKTGNPNEPHSQESKWPLFTTKEQKFIDLNTEPMKVHQRLRVQMCVFWNQFLPKLLNATETIDEAERQWKTEFHRWSSYMMHWKNQFDHYSRHESCAEL"
    buche_sequence = "MHSKVTIICIRFLFWFLLLCMLIGKSHTEDDIIIATKNGKVRGMNLTVFGGTVTAFLGIPYAQPPLGRLRFKKPQSLTKWSDIWNATKYANSCCQNIDQSFPGFHGSEMWNPNTDLSEDCLYLNVWIPAPKPKNATVLIWIYGGGFQTGTSSLHVYDGKFLARVERVIVVSMNYRVGALGFLALPGNPEAPGNMGLFDQQLALQWVQKNIAAFGGNPKSVTLFGESAGAASVSLHLLSPGSHSLFTRAILQSGSFNAPWAVTSLYEARNRTLNLAKLTGCSRENETEIIKCLRNKDPQEILLNEAFVVPYGTPLSVNFGPTVDGDFLTDMPDILLELGQFKKTQILVGVNKDEGTAFLVYGAPGFSKDNNSIITRKEFQEGLKIFFPGVSEFGKESILFHYTDWVDDQRPENYREALGDVVGDYNFICPALEFTKKFSEWGNNAFFYYFEHRSSKLPWPEWMGVMHGYEIEFVFGLPLERRDNYTKAEEILSRSIVKRWANFAKYGNPNETQNNSTSWPVFKSTEQKYLTLNTESTRIMTKLRAQQCRFWTSFFPKVLEMTGNIDEAEWEWKAGFHRWNNYMMDWKNQFNDYTSKKESCVGL"
    sequence = buche_sequence
    prot_feature = torch.FloatTensor(sequence_feature(sequence)).to(device)
    prot_features = prot_feature[:-1].unsqueeze(0).to(device)
    N = len(prot_features)
    pred_bs = model.BSP_LM(prot_features, 1, N)
    pred_bs_int = torch.argmax(pred_bs.squeeze(0), dim=-1)
    print(''.join(map(str, pred_bs_int.tolist())))
    print(torch.nonzero(pred_bs_int == 1).squeeze())


# ache
# 00000000000000000000000000000000000000000000000000000000000000000011111100111111110000000000000000000000000000101111111110010110000000000000000000000000000000000000000000000000000000000000000000011111000000000000000000000110000000000000000000000000000000000000000000000000000110101011111100000000000000000000000000000000100101111111000000000000000000000010000000000000000000000000000000000000000110000000000000000000000000000000100010011111100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
# tensor([ 66,  67,  68,  69,  70,  71,  74,  75,  76,  77,  78,  79,  80,  81,
#         110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 125, 126,
#         195, 196, 197, 198, 199, 221, 222,
#         275, 276, 278, 280, 282, 283, 284, 285, 286, 287, 320, 323, 325, 326, 327, 328, 329, 330, 331, 354,
#         395, 396, 428, 432, 435, 436, 437, 438, 439, 440], device='cuda:0')
[70, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85,
 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 129, 130,
 199, 200, 201, 202, 203, 225, 226, 279, 280, 282, 284, 286, 287, 288, 289, 290, 291,
 324, 327, 329, 330, 331, 332, 333, 334, 335, 358, 399, 400,
 432, 436, 439, 440, 441, 442, 443, 444]
# (70+71+72+73+74+75+78+79+80+81+82+83+84+85+114+116+117+118+119+120+121+122+123+124+127+129+130+199+200+201+202+203+225+226+279+280+282+284+286+287+288+289+290+291+324+327+329+330+331+332+333+334+335+358+399+400+432+436+439+440+441+442+443+444



# buche
# 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001111111001111111100000000000000000000000000001011111111000101100000000000000000000000000000000000000000000000000000000000000000000111110000000000000000000001110001011000000000000000000000000000000000000000000000000000111111000000000000000000000000000000001001111111110000000000000000000000000000000000000000000000000000000000000001100000000000000000000000000000101000100111111000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
# tensor([ 94,  95,  96,  97,  98,  99, 100, 103, 104, 105, 106, 107, 108, 109,
#         110, 139, 141, 142, 143, 144, 145, 146, 147, 148, 152, 154, 155, 224,
#         225, 226, 227, 228, 250, 251, 252, 256, 258, 259, 311, 312, 313, 314,
#         315, 316, 349, 352, 353, 354, 355, 356, 357, 358, 359, 360, 424, 425,
#         455, 457, 461, 464, 465, 466, 467, 468, 469], device='cuda:0')

# [96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 110, 111, 112,
# 226, 227, 228, 229, 230, 252, 253, 254, 258, 260, 261, 313, 314, 315, 316,
# 317, 318, 351, 354, 355, 356, 357, 358, 359, 360, 361, 362] - 2



